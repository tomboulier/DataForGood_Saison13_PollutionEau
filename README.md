# Rendre Visible la Pollution de l'Eau Potable üíß

## Contexte du Projet

Ce projet, d√©velopp√© par des b√©n√©voles de [Data For Good](https://www.dataforgood.fr/) lors de la saison 13, vise √† cr√©er une carte interactive pour [G√©n√©rations Futures](https://www.generations-futures.fr/).

L'objectif est de consolider, analyser et cartographier les donn√©es sur la qualit√© de l'eau potable en France √† partir de sources de donn√©es ouvertes.

## Structure du Projet

- `pipelines/` : Consolidation et pr√©paration des donn√©es
- `analytics/` : Analyse des donn√©es
- `webapp/` : D√©veloppement du site web interactif

## Installation

### Data Pipelines

- [Installation de Python](#installation-de-python)

Ce projet utilise [uv](https://docs.astral.sh/uv/) pour la gestion des d√©pendances Python. Il est pr√©r√©quis pour l'installation de ce projet.

Installation sur Windows

```bash
powershell -c "irm https://astral.sh/uv/install.ps1 | iex"
```

Installation sur Mac ou linux

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

Une fois install√©, il suffit de lancer la commande suivante pour installer la version de Python ad√©quate, cr√©er un environnement virtuel et installer les d√©pendances du projet.

```bash
uv sync
```

#### VSCode

A l'usage, si vous utilisez VSCode, l'environnement virtuel sera automatiquement activ√© lorsque vous ouvrirez le projet. Sinon, il suffit de l'activer manuellement avec la commande suivante :

```bash
source .venv/bin/activate
```

Ou alors, utilisez la commande `uv run ...` (au lieu de `python ...`) pour lancer un script Python. Par exemple:

```bash
uv run pipelines/run.py run build_database
```

#### Pycharm

Allez dans settings, python interpreter, add interpreter, puis selectionnez existing venv et allez chercher le path du python executable dans .venv (.venv/Scripts/Python.exe pour windows)

#### Terminal

utilisez les commandes `uv run` pour lancer un script Python depuis votre terminal

- [Installation de Node.js](#installation-de-nodejs) (pour le d√©veloppement du site web et pour l'usage de Evidence)

Pour le d√©veloppement du site web et pour l'usage de [Evidence](https://evidence.dev/), il est n√©cessaire d'installer Node.js. Pour cela, il suffit de suivre les instructions sur le [site officiel](https://nodejs.org/).

Pour installer les d√©pendances du site web, il suffit de lancer les commandes suivantes :

```bash
cd webapp
npm install
```

## Data Processing

### Package installation

Tout le code dans pipelines sera install√© en tant que package python automatiquement √† chaque uv_sync

### Comment construire la database

Une fois l'environnement python setup avec uv, vous pouvez lancer data_pipeline/run.py pour remplir la database

Le t√©l√©chargement des donn√©es peut se faire de plusieurs mani√®res :
* 1. T√©l√©chargement des donn√©es de la derni√®re ann√©e (par d√©faut)
```bash
uv run pipelines/run.py run build_database --refresh-type last
```

* 2. T√©l√©chargement de toutes les donn√©es

```bash
uv run pipelines/run.py run build_database --refresh-type all
```

* 3. T√©l√©chargement de donn√©es d'ann√©es sp√©cifiques
```bash
uv run pipelines/run.py run build_database --refresh-type custom --custom-years 2018,2024,...
```
### Cr√©ation du mod√®les de donn√©es avec dbt
#### 1. Commandes a ex√©cuter
La librarie dbt est celle choisie pour une construction rapide et simple de mod√®les de donn√©es optimis√© pour l'analytics.

üö©**Remarque** : Pour lancer chaque commande individuellement, veillez √† bien vous placer dans le dossier dbt_ (`cd dbt_`) avant de lancer les commandes.

La commande `uv run dbt deps` permet de t√©l√©charger les d√©pendances du projet dbt.
Ex√©cut√©e lors de la cr√©ation de la base de donn√©es, la commande `uv run dbt build` est une commande qui permet de r√©aliser l'ensemble des actions suivantes :
* Lancer la cr√©ation des tables issues des donn√©es brutes (`uv run dbt run`)
* R√©aliser les test de qualit√© des donn√©es (`uv run dbt test`)
* Mettre sous forme de table les fichiers csv ajout√©s dans le dossiers seeds (`uv run dbt seed`)

Une autre commande `uv run dbt docs generate` permet de g√©n√©rer la documentation des mod√®les de donn√©es renseign√©e dans les fichiers `_xxx__models.yml` au format html. L'utilisation de la commande `uv run dbt docs serve` permet de lancer un serveur local pour visualiser la documentation.

Pour plus d'informations concernant la mani√®re d'organiser un projet dbt, se r√©f√©rer √† la [documentation officielle](https://docs.getdbt.com/docs/introduction) et notamment √† la section .

#### 2. Structure des donn√©es

Les mod√®les de donn√©es sont organis√©s dans le dossier `dbt_/models`. La structure suit les recommandations de la [documentation officielle](https://docs.getdbt.com/best-practices/how-we-structure/1-guide-overview). Il est conseill√© prendre le temps la lire afin de bien comprendre la structure du projet:

* **models/staging/** : Mod√®les de donn√©es avec des transformation basiques (TRIM, REPLACE, typage, ...). Cette couche est surtout utilis√©e pour faire un √©tat des donn√©es existantes, les documenter et tester la qualit√©.
* **models/intermediate/** : Mod√®les de donn√©es avec des transformation plus complexes (GROUP BY, JOIN, WHERE, ...). Cette couche est surtout utile pour faire une jointure entre les diff√©rentes tables et faire un premier filtrage des donn√©es. Celle-ci est tr√®s utile pour de l'analyse de donn√©es
* **models/analytics/** : Mod√®les de donn√©es final, qui est requ√™ter par le site web pour construire les visualisations. Cette donn√©e est propre et la sch√©matisation des donn√©es est optimis√©e pour le chargement des visualisations.

* 4. Suppression des tables, puis t√©l√©chargement des donn√©es de la derni√®re ann√©e
```bash
uv run pipelines/run.py run build_database --refresh-type last --drop-tables
```

#### Documentation
La documentation du projet dbt est disponible sur le lien suivant: [documentation dbt](https://dataforgood.fr/13_pollution_eau/#!/overview)

### Comment t√©l√©charger la database depuis S3
Des versions de dev et de production de la db sont √† disposition sur le storage object.
Les deux fa√ßons de t√©l√©charger les databases suivantes existent.

#### via HTTPS
Le plus simple est de la t√©l√©charger via https (pas besoin de credentials):
```bash
uv run pipelines/run.py run download_database_https --env prod
```
Vous pouvez aussi simplement telecharger la donn√©e en cliquant sur le lien de telechargement suivant:  [duckdb prod database](https://pollution-eau-s3.s3.fr-par.scw.cloud/prod/database/data.duckdb)

#### via S3 (Scaleway)
Il faut bien configurer ses credentials Scaleway et son env via le fichier .env.


```bash
uv run pipelines/run.py run download_database
```

### Connection a Scaleway via boto3 pour stockage cloud

Un utils a √©t√© cr√©√© dans [storage_client.py](pipelines%2Futils%2Fstorage_client.py) pour faciliter la connection au S3 h√©berg√© sur Scaleway.

Il faut cr√©er un fichier .env dans le dossier pipelines/config avec les secrets ci dessous dedans pour que la connection fonctionne.

```text
SCW_ACCESS_KEY={ACCESS_KEY}
SCW_SECRET_KEY={SECRET_KEY}
```

Vous trouverez un exemple avec le fichier [.env.example](pipelines%2Fconfig%2F.env.example)

> ‚ö† **Attention:** Ne jamais commit les access key et secret key.

Un vaultwarden va √™tre setup pour r√©cup√©rer les secrets pour les personnes qui en ont besoin.

Le notebook [test_storage_utils.ipynb](pipelines%2Fnotebooks%2Ftest_storage_utils.ipynb) montre un exemple d'utilisation de l'utils pour charger et lire des csv sur le bucket S3 du projet.

### Data analysis

Les analyses se font via jupyter notebook

```bash
uv run jupyter notebook
```

## Tests

Pour lancer les tests, il suffit de lancer la commande suivante √† la racine du projet:

```bash
uv run pytest -s
```

L'option `-s` permet d'afficher les prints dans le terminal.

## Pre Commit

Lancer la commande suivante pour s'assurer que le code satisfait bien tous les pre commit avant de cr√©er votre pull request

```bash
pre-commit run --all-files
```

## How to contribute
Pour contribuer, il est recommand√© d'utiliser un fork du projet. Cela permet d'√©viter la gestion des demandes d'acc√®s au d√©p√¥t principal.

* Dans un premier temps, cliquez sur Fork pour r√©cup√©rer le projet dans votre espace GitHub.
* Cr√©ez votre branche de travail √† partir de la branche main, en respectant la nomenclature suivante :
  * feature/nom_de_la_feature pour une nouvelle fonctionnalit√©
  * hotfix/nom_du_hotfix pour une correction rapide
* Poussez votre code vers votre d√©p√¥t distant.
* Cr√©ez une pull request en sp√©cifiant :
  * Base repository : dataforgood/13_pollution_eau/main
  * Head repository : YourGithubAccount/13_pollution_eau/your_branch
* Pour faciliter la revue de la pull request :
  * Liez la pull request √† un ticket NocoDB en ajoutant le lien du ticket dans la description.
  * R√©digez une description d√©taill√©e de la pull request afin de fournir un maximum d‚Äôinformations sur les modifications apport√©es.
